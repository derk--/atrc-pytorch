{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Implementing a CNN for MNIST\n",
    "\n",
    "We achieved around 92% accuracy using two fully connected layers. Let's see if we can improve performance by switching to a convolutional neural network architecture. First we apply our usual imports and then setup data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# normalize the pixel values to [-1,1] with mean 0. This is really important!\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "training_data = dsets.MNIST(root=\"./data\", train = True, transform=transform, download = True)\n",
    "testing_data = dsets.MNIST(root=\"./data\", train = False, transform=transform, download = True)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_data, batch_size=len(testing_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now setup the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define and instantiate the network as a subclass of nn.Module. \n",
    "# Unlike the last example, we will simply the number of parameters and leave the num_ouputs fixed at 10.\n",
    "class cnn(nn.Module):\n",
    "\n",
    "    def __init__(self, num_input_channels):\n",
    "        super(cnn, self).__init__()\n",
    "\n",
    "        # Layer 1\n",
    "        # 1 input channel (e.g. feature map); the image\n",
    "        # 6 output channels (e.g. layer has 6 feature maps);\n",
    "        # uses 3x3 kernel over the input channel to build output channels.\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_input_channels, out_channels=10, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d((2,2))\n",
    "        \n",
    "        # Add a dropout layer to add a bit of regularization.\n",
    "        self.drop = nn.Dropout2d(p=0.25, inplace=False)\n",
    "        \n",
    "        #Why 20*10*10? See below.\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(in_features=20*10*10, out_features=100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(100,10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    # x: input to the network\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass x through conv1 using relu activations.\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)  \n",
    "        x = self.maxpool(x) \n",
    "        x = self.drop(x)\n",
    "        ## We need to flatten all of the feature map activations into a column vector to pass into the fc layers.\n",
    "        ## Why is it 20*10*10? See below.\n",
    "        x = x.view(-1, 20*10*10) \n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you should keep track of the spatial size of the output of each convolutional layer to track how big your kernel, stride, and padding has to be. The output volume of a conv layer is a function of the input volume size (W), the receptive field size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. You should convince yourself that the correct formula for calculating how many neurons “fit” is given by: \n",
    "\n",
    "$$\\frac{W−F+2P}{S} + 1$$\n",
    "\n",
    "So if the input to the CNN layer is 7x7, the kernel size is 3x3, stride of 1, and no 0-padding, the output volume with have dimensions 5x5 (as $(7-3+0)/1 + 1 = 5$). Note that the depth of the input (e.g. number of color channels or feature maps at previous layer) is irrelevant: as many feature maps can be combined to produce as many feature maps as you want at the subsequent layer.\n",
    "\n",
    "Lets look at the network above: for conv1; input is 28x28, kernel is 5x5, side of 1, no 0-padding. So the output feature maps will have shape $(28-5+0)/1 + 1 = 24$, so 24x24. \n",
    "\n",
    "In conv2, input is 24x24, kernel is 5x5, stride of 1, no 0-adding. Each of the 16 output feature maps will have dimension $(24-5+0)/1 + 1 = 20$, so 20x20. \n",
    "\n",
    "The output of conv2 goes through a maxpool with 2x2 kernel with stride of 2. The output of the maxpool will thus have shape $(20-2+0)/2 + 1 = 10$, so 10x10. \n",
    "\n",
    "Note that we need to reshape the activations in the 20 10x10 feature maps in the maxpool layer into a single vector as input to the fully\n",
    "connected layer. According to these calculations, we should have $10*10$ features per feature maps, and there are 20 feature maps after maxpool, so total size of the fc3 input is $10*10*20$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d (1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d (10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (drop): Dropout2d(p=0.25)\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=2000, out_features=100)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc4): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=10)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "torch.Size([10, 1, 5, 5])\n",
      "torch.Size([10])\n",
      "torch.Size([20, 10, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([100, 2000])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "number of network parameters: 206390\n"
     ]
    }
   ],
   "source": [
    "the_net = cnn(num_input_channels=1) ##MNIST digits are grayscale. \n",
    "print(the_net)\n",
    "\n",
    "num_parameters = 0\n",
    "for x in the_net.parameters():\n",
    "    print x.shape\n",
    "    \n",
    "for x in the_net.parameters():\n",
    "    num_params = 1\n",
    "    for dim_size in x.shape:\n",
    "        num_params *= dim_size\n",
    "    num_parameters += num_params\n",
    "    \n",
    "print('number of network parameters: %i' % (num_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 206,390 weights to learn (recall we only had about 8000 in the two fully connected layer network)!\n",
    "\n",
    "**Let's use the GPU for this much bigger network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_net = cnn(num_input_channels=1) ##MNIST digits are grayscale. \n",
    "the_net.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(the_net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train! **Note that all Variables go onto the GPU**. Also observe that we need to move some variables back to the **CPU** in order to do comparisons with data stored in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, minibatch 0. Loss: 2.3048.\n",
      "At epoch 0, minibatch 300. Loss: 1.6442.\n",
      "At epoch 0, minibatch 600. Loss: 0.3903.\n",
      "At epoch 0, minibatch 900. Loss: 0.2438.\n",
      "Epoch 0 finished! It took: 5.7396 seconds\n",
      "Accuracy of the network on the 10000 test images: 92.03 %\n",
      "At epoch 1, minibatch 0. Loss: 0.4225.\n",
      "At epoch 1, minibatch 300. Loss: 0.1288.\n",
      "At epoch 1, minibatch 600. Loss: 0.1220.\n",
      "At epoch 1, minibatch 900. Loss: 0.1005.\n",
      "Epoch 1 finished! It took: 5.1696 seconds\n",
      "Accuracy of the network on the 10000 test images: 94.77 %\n",
      "At epoch 2, minibatch 0. Loss: 0.1395.\n",
      "At epoch 2, minibatch 300. Loss: 0.1230.\n",
      "At epoch 2, minibatch 600. Loss: 0.0808.\n",
      "At epoch 2, minibatch 900. Loss: 0.1665.\n",
      "Epoch 2 finished! It took: 5.1457 seconds\n",
      "Accuracy of the network on the 10000 test images: 96.37 %\n",
      "At epoch 3, minibatch 0. Loss: 0.2040.\n",
      "At epoch 3, minibatch 300. Loss: 0.2229.\n",
      "At epoch 3, minibatch 600. Loss: 0.1274.\n",
      "At epoch 3, minibatch 900. Loss: 0.0636.\n",
      "Epoch 3 finished! It took: 5.1681 seconds\n",
      "Accuracy of the network on the 10000 test images: 96.60 %\n",
      "At epoch 4, minibatch 0. Loss: 0.0480.\n",
      "At epoch 4, minibatch 300. Loss: 0.0949.\n",
      "At epoch 4, minibatch 600. Loss: 0.1294.\n",
      "At epoch 4, minibatch 900. Loss: 0.0596.\n",
      "Epoch 4 finished! It took: 5.1741 seconds\n",
      "Accuracy of the network on the 10000 test images: 97.03 %\n",
      "At epoch 5, minibatch 0. Loss: 0.0400.\n",
      "At epoch 5, minibatch 300. Loss: 0.0719.\n",
      "At epoch 5, minibatch 600. Loss: 0.0686.\n",
      "At epoch 5, minibatch 900. Loss: 0.0755.\n",
      "Epoch 5 finished! It took: 5.1447 seconds\n",
      "Accuracy of the network on the 10000 test images: 97.39 %\n",
      "At epoch 6, minibatch 0. Loss: 0.1184.\n",
      "At epoch 6, minibatch 300. Loss: 0.0253.\n",
      "At epoch 6, minibatch 600. Loss: 0.0633.\n",
      "At epoch 6, minibatch 900. Loss: 0.0850.\n",
      "Epoch 6 finished! It took: 5.1574 seconds\n",
      "Accuracy of the network on the 10000 test images: 97.15 %\n",
      "At epoch 7, minibatch 0. Loss: 0.0591.\n",
      "At epoch 7, minibatch 300. Loss: 0.0624.\n",
      "At epoch 7, minibatch 600. Loss: 0.1687.\n",
      "At epoch 7, minibatch 900. Loss: 0.0601.\n",
      "Epoch 7 finished! It took: 5.1540 seconds\n",
      "Accuracy of the network on the 10000 test images: 97.89 %\n",
      "At epoch 8, minibatch 0. Loss: 0.0534.\n",
      "At epoch 8, minibatch 300. Loss: 0.1501.\n",
      "At epoch 8, minibatch 600. Loss: 0.0273.\n",
      "At epoch 8, minibatch 900. Loss: 0.0848.\n",
      "Epoch 8 finished! It took: 5.2058 seconds\n",
      "Accuracy of the network on the 10000 test images: 97.84 %\n",
      "At epoch 9, minibatch 0. Loss: 0.0086.\n",
      "At epoch 9, minibatch 300. Loss: 0.2909.\n",
      "At epoch 9, minibatch 600. Loss: 0.1293.\n",
      "At epoch 9, minibatch 900. Loss: 0.0467.\n",
      "Epoch 9 finished! It took: 5.1663 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.02 %\n",
      "At epoch 10, minibatch 0. Loss: 0.0432.\n",
      "At epoch 10, minibatch 300. Loss: 0.0084.\n",
      "At epoch 10, minibatch 600. Loss: 0.0091.\n",
      "At epoch 10, minibatch 900. Loss: 0.0217.\n",
      "Epoch 10 finished! It took: 5.1686 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.00 %\n",
      "At epoch 11, minibatch 0. Loss: 0.0656.\n",
      "At epoch 11, minibatch 300. Loss: 0.0678.\n",
      "At epoch 11, minibatch 600. Loss: 0.0405.\n",
      "At epoch 11, minibatch 900. Loss: 0.0919.\n",
      "Epoch 11 finished! It took: 5.1737 seconds\n",
      "Accuracy of the network on the 10000 test images: 97.84 %\n",
      "At epoch 12, minibatch 0. Loss: 0.0222.\n",
      "At epoch 12, minibatch 300. Loss: 0.0406.\n",
      "At epoch 12, minibatch 600. Loss: 0.0080.\n",
      "At epoch 12, minibatch 900. Loss: 0.0365.\n",
      "Epoch 12 finished! It took: 5.1298 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.02 %\n",
      "At epoch 13, minibatch 0. Loss: 0.0135.\n",
      "At epoch 13, minibatch 300. Loss: 0.0924.\n",
      "At epoch 13, minibatch 600. Loss: 0.0756.\n",
      "At epoch 13, minibatch 900. Loss: 0.0273.\n",
      "Epoch 13 finished! It took: 5.1780 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.15 %\n",
      "At epoch 14, minibatch 0. Loss: 0.0071.\n",
      "At epoch 14, minibatch 300. Loss: 0.0640.\n",
      "At epoch 14, minibatch 600. Loss: 0.0167.\n",
      "At epoch 14, minibatch 900. Loss: 0.0032.\n",
      "Epoch 14 finished! It took: 5.1973 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.21 %\n",
      "At epoch 15, minibatch 0. Loss: 0.2209.\n",
      "At epoch 15, minibatch 300. Loss: 0.0159.\n",
      "At epoch 15, minibatch 600. Loss: 0.1052.\n",
      "At epoch 15, minibatch 900. Loss: 0.0211.\n",
      "Epoch 15 finished! It took: 5.1494 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.26 %\n",
      "At epoch 16, minibatch 0. Loss: 0.0044.\n",
      "At epoch 16, minibatch 300. Loss: 0.0180.\n",
      "At epoch 16, minibatch 600. Loss: 0.0376.\n",
      "At epoch 16, minibatch 900. Loss: 0.0104.\n",
      "Epoch 16 finished! It took: 5.1382 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.29 %\n",
      "At epoch 17, minibatch 0. Loss: 0.0118.\n",
      "At epoch 17, minibatch 300. Loss: 0.0235.\n",
      "At epoch 17, minibatch 600. Loss: 0.0992.\n",
      "At epoch 17, minibatch 900. Loss: 0.0205.\n",
      "Epoch 17 finished! It took: 5.1401 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.30 %\n",
      "At epoch 18, minibatch 0. Loss: 0.0107.\n",
      "At epoch 18, minibatch 300. Loss: 0.0809.\n",
      "At epoch 18, minibatch 600. Loss: 0.0470.\n",
      "At epoch 18, minibatch 900. Loss: 0.0509.\n",
      "Epoch 18 finished! It took: 5.1776 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.34 %\n",
      "At epoch 19, minibatch 0. Loss: 0.0264.\n",
      "At epoch 19, minibatch 300. Loss: 0.0684.\n",
      "At epoch 19, minibatch 600. Loss: 0.1705.\n",
      "At epoch 19, minibatch 900. Loss: 0.0349.\n",
      "Epoch 19 finished! It took: 5.1347 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.40 %\n",
      "At epoch 20, minibatch 0. Loss: 0.0132.\n",
      "At epoch 20, minibatch 300. Loss: 0.0790.\n",
      "At epoch 20, minibatch 600. Loss: 0.0032.\n",
      "At epoch 20, minibatch 900. Loss: 0.0216.\n",
      "Epoch 20 finished! It took: 5.1096 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.28 %\n",
      "At epoch 21, minibatch 0. Loss: 0.0155.\n",
      "At epoch 21, minibatch 300. Loss: 0.0221.\n",
      "At epoch 21, minibatch 600. Loss: 0.0474.\n",
      "At epoch 21, minibatch 900. Loss: 0.0093.\n",
      "Epoch 21 finished! It took: 5.1570 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.36 %\n",
      "At epoch 22, minibatch 0. Loss: 0.0075.\n",
      "At epoch 22, minibatch 300. Loss: 0.0110.\n",
      "At epoch 22, minibatch 600. Loss: 0.0410.\n",
      "At epoch 22, minibatch 900. Loss: 0.0077.\n",
      "Epoch 22 finished! It took: 5.1735 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.52 %\n",
      "At epoch 23, minibatch 0. Loss: 0.0046.\n",
      "At epoch 23, minibatch 300. Loss: 0.0027.\n",
      "At epoch 23, minibatch 600. Loss: 0.0287.\n",
      "At epoch 23, minibatch 900. Loss: 0.0081.\n",
      "Epoch 23 finished! It took: 5.1970 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.30 %\n",
      "At epoch 24, minibatch 0. Loss: 0.0038.\n",
      "At epoch 24, minibatch 300. Loss: 0.0104.\n",
      "At epoch 24, minibatch 600. Loss: 0.0363.\n",
      "At epoch 24, minibatch 900. Loss: 0.1318.\n",
      "Epoch 24 finished! It took: 5.1256 seconds\n",
      "Accuracy of the network on the 10000 test images: 98.36 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    start = timer()\n",
    "    for batch_num , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "    \n",
    "        the_batch = Variable(minibatch_of_images).cuda()\n",
    "        labels = Variable(minibatch_of_labels).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = the_net(the_batch)\n",
    "        \n",
    "        loss = loss_function(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # we want to check the accuracy with test dataset every 300 iterations.\n",
    "        if batch_num % 300 == 0:\n",
    "            print(\"At epoch %i, minibatch %i. Loss: %.4f.\" % (epoch, batch_num, loss.data[0]))\n",
    "            \n",
    "    end = timer()\n",
    "    print(\"Epoch %i finished! It took: %.4f seconds\" % (epoch, end - start))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = the_net(Variable(images).cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "    print('Accuracy of the network on the %d test images: %.2f %%' % (total, 100.0 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 2% test error is awesome! \n",
    "\n",
    "BTW -- how long would an epoch take on a CPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 0. Loss: 2.3033.\n",
      "At minibatch 300. Loss: 1.9801.\n",
      "At minibatch 600. Loss: 0.5785.\n",
      "At minibatch 900. Loss: 0.4037.\n",
      "One epoch on the CPU took: 26.3057 seconds\n"
     ]
    }
   ],
   "source": [
    "the_net = cnn(num_input_channels=1) ##MNIST digits are grayscale. \n",
    "optimizer = torch.optim.SGD(the_net.parameters(), lr=0.01)\n",
    "start = timer()\n",
    "for batch_num , (minibatch_of_images, minibatch_of_labels) in enumerate(train_loader):\n",
    "\n",
    "    the_batch = Variable(minibatch_of_images)\n",
    "    labels = Variable(minibatch_of_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = the_net(the_batch)\n",
    "\n",
    "    loss = loss_function(output, labels)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # we want to check the accuracy with test dataset every 300 iterations.\n",
    "    if batch_num % 300 == 0:\n",
    "        print(\"At minibatch %i. Loss: %.4f.\" % (batch_num, loss.data[0]))\n",
    "\n",
    "end = timer()\n",
    "print(\"One epoch on the CPU took: %.4f seconds\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So about 5x the time**... and this is a relatively small CNN! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
